{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Landscape indicator computation notebook\n",
    "\n",
    "Scripts for preprocessing and generating landscape indicators using the 50km sLUC methodology\n",
    " - Deforesation, \n",
    " - Deforestation carbon, and \n",
    " - Cropland expansion in natural lands\n",
    "\n",
    "## Methodology\n",
    "\n",
    "See doc: https://docs.google.com/document/d/1s36r6jSCGkmgQAf4Rg4X36oYYxeDnT0jvvO5yPxnDE4/edit\n",
    "\n",
    "## WARNING\n",
    "This notebook is not idempotent! \n",
    "\n",
    "You can run it all the way to the end and preview results as visuals.\n",
    "\n",
    "The full analysis requires the notebook to be run at least 3 times over a period of >1 day \n",
    "to cache intermediate calculations if starting from a clean working directory:\n",
    " - First, we create or resample source layers to 100m.\n",
    " - Then we do the overlay analysis to identify deforestation, cropland expansion, etc.\n",
    " - Then we use these outputs to compute the final kerneled layers at 1000m.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vr4Jtu-jQbuQ"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import ee\n",
    "import eeUtil\n",
    "import geemap.foliumap as gmap\n",
    "from ee import EEException\n",
    "\n",
    "# Initialize Earth Engine\n",
    "PROJECT = os.getenv(\"GEE_PROJECT\")\n",
    "GEE_JSON = os.getenv(\"GEE_JSON\")\n",
    "\n",
    "assert PROJECT is not None, \"Please set GEE_PROJECT environment variable\"\n",
    "assert (\n",
    "    GEE_JSON is not None\n",
    "), \"Please set GEE_JSON environment variable with service account credentials\"\n",
    "\n",
    "eeUtil.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fbxXNfC4_rIs"
   },
   "outputs": [],
   "source": [
    "# Constants / options for the landscape indicators analysis\n",
    "PROJECTION = \"EPSG:4326\"\n",
    "WORKING_FOLDER = f\"projects/{PROJECT}/assets/landscape_indicators_20230821\"\n",
    "SRC_FOLDER = \"projects/ee-fgassert/assets/landscape_indicators_source\"\n",
    "WORLD_GEOM = ee.Geometry.Polygon(\n",
    "    coords=[[[-180, -85], [-180, 85], [180, 85], [180, -85], [-180, -85]]],\n",
    "    proj=PROJECTION,\n",
    "    geodesic=False,\n",
    ")\n",
    "EXPORT_BUCKET = os.getenv(\"GEE_BUCKET\")\n",
    "EXPORT_PREFIX = \"landscape_indicators_20230821\"\n",
    "\n",
    "TARGET_YEAR = 2022  # the target year for analysis\n",
    "START_YEAR = TARGET_YEAR - 20  # the start year for deforestation analysis\n",
    "\n",
    "# scales in meters\n",
    "ANALYSIS_SCALE = 100  # the working scale for overlays in the main analysis\n",
    "KERNEL_SCALE = 1000  # the scale for the kernel analysis\n",
    "KERNEL_RADIUS = 50000\n",
    "\n",
    "ALL_INPUT_LAYERS = [\n",
    "    f\"tree_loss_to{TARGET_YEAR}\",\n",
    "    \"nonnat_forest\",\n",
    "    \"intact_forests\",\n",
    "    \"primary_tropical_forests\",\n",
    "    \"burned_area_mask\",\n",
    "    \"disturbance\",\n",
    "    \"carbon_filled_2000\",\n",
    "    \"esri_cropland_2020\",\n",
    "    f\"esri_cropland_{TARGET_YEAR}\",\n",
    "    \"natural_lands\",\n",
    "]\n",
    "\n",
    "# layers that must be computed at a higher resolution before downsampling for the kernel\n",
    "PRE_KERNEL_LAYERS = [\n",
    "    f\"tree_loss_to{TARGET_YEAR}\",\n",
    "    \"deforest\",\n",
    "    \"deforest_carbon\",\n",
    "    \"natcrop_expansion\",\n",
    "    \"natcrop_reduction\",\n",
    "    \"nonnatural_excl_builtwater\",\n",
    "]\n",
    "\n",
    "#### ASSETS ########\n",
    "\n",
    "####################\n",
    "# derived from RESOLVE ecoregions\n",
    "NON_SUBTROPIC_ASSET = f\"{SRC_FOLDER}/not_subtropic\"\n",
    "# Noon et al. 2022 vulnerable carbon total for 2010\n",
    "CARBON_ASSET = f\"{SRC_FOLDER}/Vulnerable_C_Total_2010\"\n",
    "# ESA CCI land cover for 2010\n",
    "ESA_CCI_ASSET = f\"{SRC_FOLDER}/ESACCI_LC_300m_P1Y_2010_v207\"\n",
    "\n",
    "####################\n",
    "# 3rd party hosted assets\n",
    "HANSEN_ASSET = \"UMD/hansen/global_forest_change_2022_v1_10\"\n",
    "FOREST_DYNAMICS_ASSET = \"projects/glad/GLCLU2020/Forest_type\"\n",
    "INTACT_FORESTS_ASSET = \"users/potapovpeter/IFL_2000\"\n",
    "PRIMARY_TROPICAL_ASSET = \"UMD/GLAD/PRIMARY_HUMID_TROPICAL_FORESTS/v1/2001\"\n",
    "\n",
    "ESRI_LULC_IC = \"projects/sat-io/open-datasets/landcover/ESRI_Global-LULC_10m_TS\"\n",
    "CROPLAND_2019_IC = \"users/potapovpeter/Global_cropland_2019\"\n",
    "SBTN_ASSET = \"projects/wri-datalab/SBTN/natLands_beta/naturalLands_allClasses_20230516\"\n",
    "\n",
    "FIRECCI_IC = \"ESA/CCI/FireCCI/5_1\"\n",
    "MODIS_BA_IC = \"MODIS/006/MCD64A1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create working folder if not exists\n",
    "eeUtil.createFolder(WORKING_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nhlPlJbXJHiR"
   },
   "outputs": [],
   "source": [
    "def get_sbtn_layers():\n",
    "    \"\"\"Get relevant SBTN layers at reduced resolution using mean pyramiding.\n",
    "\n",
    "    All Classes values:\n",
    "    Natural:     2 = natural forests, 3 = natural short vegetation, 4 = natural water, 5 = mangroves\n",
    "                 6 = bare, 7 = snow, 8 = wet natural forests, 9 = natural peat forests,\n",
    "                 10 = wet natural short vegetation, 11 = natural peat short vegetation\n",
    "    Non-Natural: 12 = cropland, 13 = built, 14 = non-natural tree cover, 15 = non-natural short\n",
    "                 vegetation, 16 = non-natural water, 17 = wet non-natural forests,\n",
    "                 18 = non-natural peat forests, 19 = wet non-natural short vegetation,\n",
    "                 20 = non-natural peat short vegetation.\n",
    "    \"\"\"\n",
    "    sbtn_all_classes = ee.Image(SBTN_ASSET)\n",
    "    return {\n",
    "        \"natural_lands\": sbtn_all_classes.lte(11),\n",
    "        \"nonnat_forest\": sbtn_all_classes.eq(ee.Image([14, 17, 18])).reduce(\"anyNonZero\"),\n",
    "        \"nonnatural_excl_builtwater\": sbtn_all_classes.gte(12)\n",
    "        .And(sbtn_all_classes.neq(13))\n",
    "        .And(sbtn_all_classes.neq(16)),\n",
    "        \"available_area_mask\": sbtn_all_classes.eq([4, 13, 16]).reduce(\"anyNonZero\").Not(),\n",
    "    }\n",
    "\n",
    "\n",
    "def get_mask():\n",
    "    \"\"\"Get mask of available area at target resolution.\"\"\"\n",
    "    return ee.Image(SBTN_ASSET).eq([4, 13, 16]).reduce(\"anyNonZero\").Not()\n",
    "\n",
    "\n",
    "def get_tree_loss():\n",
    "    \"\"\"Caches and returns Hansen tree loss at reduced resolution using mean pyramiding.\"\"\"\n",
    "    hanson_data = ee.Image(HANSEN_ASSET)\n",
    "    loss_year = hanson_data.select(3)\n",
    "    loss_portion = hanson_data.select(1)\n",
    "\n",
    "    return {\n",
    "        \"tree_loss_allyrs\": loss_portion,\n",
    "        f\"tree_loss_to{TARGET_YEAR}\": loss_portion.multiply(loss_year.gte(START_YEAR - 2000)),\n",
    "    }\n",
    "\n",
    "\n",
    "def get_esri_cropland(start_year=2020, end_year=TARGET_YEAR, lag_years=3):\n",
    "    \"\"\"Gets Esri cropland at reduced resolution using mean pyramiding.\n",
    "\n",
    "    Each year represents the maximum cropland area in the previous `lag_years`.\n",
    "    \"\"\"\n",
    "\n",
    "    esri_lulc = ee.ImageCollection(ESRI_LULC_IC)\n",
    "    esri_crop_annual = ee.List.sequence(start_year - lag_years, end_year).map(\n",
    "        lambda y: (\n",
    "            esri_lulc.filterDate(ee.Date.fromYMD(y, 1, 1), ee.Date.fromYMD(y, 12, 31))\n",
    "            .mosaic()\n",
    "            .eq(5)\n",
    "        )\n",
    "    )\n",
    "    esri_crop_lagged = ee.List.sequence(0, esri_crop_annual.length().subtract(lag_years + 1)).map(\n",
    "        lambda i: (\n",
    "            ee.ImageCollection(esri_crop_annual.slice(i, ee.Number(i).add(lag_years + 1))).reduce(\n",
    "                \"max\"\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    layers = {}\n",
    "    for i in range(end_year - start_year + 1):\n",
    "        y = start_year + i\n",
    "        layers[f\"esri_cropland_{y}\"] = ee.Image(esri_crop_lagged.get(i))\n",
    "\n",
    "    return layers\n",
    "\n",
    "\n",
    "def get_forest_dynamics():\n",
    "    \"\"\"Gets Potapov forest dynamics at reduced resolution using mean pyramiding.\"\"\"\n",
    "    forest_dynamics = ee.Image(FOREST_DYNAMICS_ASSET)\n",
    "    return {\"gain\": forest_dynamics.eq(3), \"disturbance\": forest_dynamics.eq(4)}\n",
    "\n",
    "\n",
    "def get_burned_area(kernel_radius=250):\n",
    "    \"\"\"Gets FireCII and Modis Burned Area products\n",
    "\n",
    "    FireCCI is a bit more accurate and reported at 250m but only runs through 2020\n",
    "    MODIS Burned Area is reported at 500m but is operationally near real-time\n",
    "\n",
    "    We run both though a kernel to smooth out the edges and expand the filter region\n",
    "    \"\"\"\n",
    "    firecci = ee.ImageCollection(FIRECCI_IC)\n",
    "    modis_ba = ee.ImageCollection(MODIS_BA_IC)\n",
    "\n",
    "    firecci = (\n",
    "        firecci.filterDate(ee.Date.fromYMD(START_YEAR, 1, 1), ee.Date.fromYMD(TARGET_YEAR, 12, 31))\n",
    "        .select(0)\n",
    "        .mosaic()\n",
    "        .setDefaultProjection(firecci.first().select(0).projection())\n",
    "    )\n",
    "    modis_ba = (\n",
    "        modis_ba.filterDate(ee.Date.fromYMD(START_YEAR, 1, 1), ee.Date.fromYMD(TARGET_YEAR, 12, 31))\n",
    "        .select(0)\n",
    "        .mosaic()\n",
    "        .setDefaultProjection(modis_ba.first().select(0).projection())\n",
    "    )\n",
    "\n",
    "    layers = {\"firecci\": firecci, \"modis_ba\": modis_ba, \"burned_area_mask\": firecci.Or(modis_ba)}\n",
    "\n",
    "    return {\n",
    "        k: image.reduceNeighborhood(\n",
    "            \"sum\", ee.Kernel.square(kernel_radius, \"meters\"), None, False, \"boxcar\"\n",
    "        ).gt(0)\n",
    "        for k, image in layers.items()\n",
    "    }\n",
    "\n",
    "\n",
    "def get_vulnerable_carbon_filled():\n",
    "    \"\"\"Computes a filled version of the Noon et al. vulnerable carbon layer for 2000.\n",
    "\n",
    "    The original layer is at 300m for the year 2010. We fill in forest areas for the year 2000\n",
    "    by looking at the difference in forest cover, and filling with the mean carbon value for forests\n",
    "    in a 10km radius around the forest pixel.\n",
    "    \"\"\"\n",
    "    carbon = ee.Image(CARBON_ASSET).unmask()\n",
    "    tree_cover_2000 = ee.Image(HANSEN_ASSET).select(1).unmask()\n",
    "    esa_cci = ee.Image(ESA_CCI_ASSET).unmask()\n",
    "\n",
    "    cci_tree_cover_2010 = (\n",
    "        esa_cci.gte(40).And(esa_cci.lt(100)).Or(esa_cci.eq(160)).Or(esa_cci.eq(170))\n",
    "    )\n",
    "    tree2000_not_2010 = tree_cover_2000.And(cci_tree_cover_2010.Not())\n",
    "    carbon_kernel_mean = (\n",
    "        carbon.multiply(cci_tree_cover_2010)\n",
    "        .selfMask()\n",
    "        .reduceNeighborhood(\"mean\", ee.Kernel.square(10000, \"meters\"), \"mask\", False, \"boxcar\")\n",
    "    )\n",
    "    carbon_filled_2000 = carbon.unmask().where(tree2000_not_2010, carbon_kernel_mean).selfMask()\n",
    "\n",
    "    return {\"carbon\": carbon, \"carbon_filled_2000\": carbon_filled_2000}\n",
    "\n",
    "\n",
    "def get_intact_forests():\n",
    "    \"\"\"Get intact and primary humid tropical forest layers at 100m resolution.\"\"\"\n",
    "    return {\n",
    "        \"intact_forests\": ee.Image(INTACT_FORESTS_ASSET),\n",
    "        \"primary_tropical_forests\": ee.Image(PRIMARY_TROPICAL_ASSET),\n",
    "    }\n",
    "\n",
    "\n",
    "def get_cropland_2019():\n",
    "    \"\"\"Get potapov cropland layer for 2019 at target resolution.\"\"\"\n",
    "    return {\"cropland_2019\": ee.ImageCollection(CROPLAND_2019_IC).mosaic()}\n",
    "\n",
    "\n",
    "def get_all_input_layers(cache=False):\n",
    "    \"\"\"Get all layers used in the analysis.\"\"\"\n",
    "    return {\n",
    "        **get_sbtn_layers(),\n",
    "        **get_esri_cropland(),\n",
    "        **get_tree_loss(),\n",
    "        **get_intact_forests(),\n",
    "        **get_vulnerable_carbon_filled(),\n",
    "        **get_burned_area(),\n",
    "        **get_forest_dynamics(),\n",
    "        **get_cropland_2019(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bz4YyUmCVs2E"
   },
   "outputs": [],
   "source": [
    "# functions for logical operations on 0-1 floating point rasters\n",
    "\n",
    "\n",
    "def and_assume_overlap(*args):\n",
    "    return ee.Image([*args]).reduce(\"min\")\n",
    "\n",
    "\n",
    "def and_assume_uniform(*args):\n",
    "    return ee.Image([*args]).reduce(\"product\")\n",
    "\n",
    "\n",
    "def and_assume_disjoint(*args):\n",
    "    im = ee.Image([*args]).reduce(\"sum\")\n",
    "    return im.subtract(im.mask()).max(0)\n",
    "\n",
    "\n",
    "def or_assume_overlap(*args):\n",
    "    return ee.Image([*args]).reduce(\"max\")\n",
    "\n",
    "\n",
    "def or_assume_uniform(*args):\n",
    "    return inverse(inverse(args).reduce(\"product\"))\n",
    "\n",
    "\n",
    "def or_assume_disjoint(*args):\n",
    "    return ee.Image([*args]).reduce(\"sum\").min(1)\n",
    "\n",
    "\n",
    "def inverse(a):\n",
    "    return ee.Image(a).subtract(1).multiply(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_deforest(layers=None):\n",
    "    \"\"\"Calculate deforestation from forest loss excluding likely types of non-deforestation loss.\n",
    "\n",
    "    Returns the input layers plus the new 'deforest' layer\n",
    "    \"\"\"\n",
    "    layers = layers or get_all_input_layers()\n",
    "\n",
    "    tree_loss = layers[f\"tree_loss_to{TARGET_YEAR}\"]\n",
    "    non_nat_forest = layers[\"nonnat_forest\"]\n",
    "    not_primary_forest = inverse(\n",
    "        or_assume_overlap(layers[\"intact_forests\"], layers[\"primary_tropical_forests\"])\n",
    "    )\n",
    "    disturbance = layers[\"disturbance\"]\n",
    "    burned_area = layers[\"burned_area_mask\"]\n",
    "    not_subtropic = ee.Image(NON_SUBTROPIC_ASSET)\n",
    "\n",
    "    forest_ok = and_assume_overlap(non_nat_forest, not_primary_forest)\n",
    "    disturbance_ok = and_assume_overlap(disturbance, not_primary_forest)\n",
    "    burned_area_ok = and_assume_uniform(burned_area, not_subtropic)\n",
    "    loss_ok = or_assume_overlap(forest_ok, disturbance_ok, burned_area_ok)\n",
    "\n",
    "    deforest = tree_loss.subtract(loss_ok).max(0)\n",
    "\n",
    "    return {**layers, \"deforest\": deforest}\n",
    "\n",
    "\n",
    "def compute_deforest_carbon(layers=None):\n",
    "    \"\"\"Calculate the carbon loss from deforestation.\n",
    "\n",
    "    Returns the input layers plus the new 'deforest_carbon' layer\n",
    "    \"\"\"\n",
    "    layers = layers or compute_deforest()\n",
    "\n",
    "    deforest = layers[\"deforest\"]\n",
    "    deforest_carbon = deforest.multiply(layers[\"carbon_filled_2000\"])\n",
    "\n",
    "    return {**layers, \"deforest_carbon\": deforest_carbon}\n",
    "\n",
    "\n",
    "def compute_cropland_expansion(layers=None):\n",
    "    \"\"\"Compute cropland expansion and reduction layers from Esri cropland data\n",
    "\n",
    "    Mask to natural lands to get natural cropland expansion and reduction layers.\n",
    "    \"\"\"\n",
    "    layers = layers or get_all_input_layers()\n",
    "\n",
    "    cropland_end = layers[f\"esri_cropland_{TARGET_YEAR}\"]\n",
    "    cropland_2020 = layers[\"esri_cropland_2020\"]\n",
    "    natural = layers[\"natural_lands\"]\n",
    "    cropland_expansion = cropland_end.subtract(cropland_2020).max(0)\n",
    "    cropland_reduction = cropland_2020.subtract(cropland_end).max(0)\n",
    "    natcrop_expansion = and_assume_disjoint(cropland_expansion, natural)\n",
    "    natcrop_reduction = and_assume_disjoint(cropland_reduction, natural)\n",
    "\n",
    "    return {\n",
    "        **layers,\n",
    "        \"cropland_expansion\": cropland_expansion,\n",
    "        \"cropland_reduction\": cropland_reduction,\n",
    "        \"natcrop_expansion\": natcrop_expansion,\n",
    "        \"natcrop_reduction\": natcrop_reduction,\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_derived_layers(input_layers=None):\n",
    "    \"\"\"Compute the derived land use change layers before kernels\"\"\"\n",
    "    layers = input_layers or get_all_input_layers()\n",
    "    layers = compute_deforest(layers)\n",
    "    layers = compute_deforest_carbon(layers)\n",
    "    layers = compute_cropland_expansion(layers)\n",
    "\n",
    "    return {k: layers[k] for k in PRE_KERNEL_LAYERS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_kernels(derived_layers=None, kernel_radius=KERNEL_RADIUS):\n",
    "    \"\"\"computed kerneled layers\"\"\"\n",
    "    layers = derived_layers or compute_derived_layers()\n",
    "\n",
    "    # mask all layers to only non-urban non-water areas\n",
    "    mask = get_mask()\n",
    "    kernel_layers = {\n",
    "        \"tree_loss_kernel\": layers[f\"tree_loss_to{TARGET_YEAR}\"],\n",
    "        \"deforest_kernel\": layers[\"deforest\"],\n",
    "        \"deforest_carbon_kernel\": layers[\"deforest_carbon\"],\n",
    "        \"natcrop_expansion_kernel\": layers[\"natcrop_expansion\"],\n",
    "        \"natcrop_reduction_kernel\": layers[\"natcrop_reduction\"],\n",
    "        \"nonnatural_kernel\": layers[\"nonnatural_excl_builtwater\"],\n",
    "    }\n",
    "\n",
    "    kernel = ee.Kernel.circle(radius=kernel_radius, units=\"meters\")\n",
    "    for k, image in kernel_layers.items():\n",
    "        image = image.updateMask(mask)\n",
    "        kernel_layers[k] = image.reduceNeighborhood(\"mean\", kernel, \"mask\", False)\n",
    "\n",
    "    return kernel_layers\n",
    "\n",
    "\n",
    "def compute_indicators(kernel_layers=None):\n",
    "    \"\"\"Compute final landscape indicators\"\"\"\n",
    "    layers = kernel_layers or compute_kernels()\n",
    "\n",
    "    natural_crop_reduction_kernel = layers[\"natcrop_reduction_kernel\"]\n",
    "    natural_crop_conversion_kernel = layers[\"natcrop_expansion_kernel\"]\n",
    "    deforest_kernel = layers[\"deforest_kernel\"]\n",
    "    deforest_carbon_kernel = layers[\"deforest_carbon_kernel\"]\n",
    "    non_nat_kernel = layers[\"nonnatural_kernel\"]\n",
    "    forest_loss_kernel = layers[\"tree_loss_kernel\"]\n",
    "\n",
    "    # avoid divide by zero errors and cap at 1ha/ha\n",
    "    human_lu = non_nat_kernel.add(0.000001)\n",
    "\n",
    "    tree_loss_by_human_lu = forest_loss_kernel.divide(human_lu).min(1).max(0)\n",
    "    deforest_by_human_lu = deforest_kernel.divide(human_lu)\n",
    "\n",
    "    # scale carbon by excess deforest to cap at 1ha/ha\n",
    "    excess_deforest_per_ha_human_lu = deforest_by_human_lu.where(deforest_by_human_lu.lt(1), 1)\n",
    "    deforest_by_human_lu = deforest_by_human_lu.min(1)\n",
    "    deforest_carbon_by_human_lu = deforest_carbon_kernel.divide(human_lu).divide(\n",
    "        excess_deforest_per_ha_human_lu\n",
    "    )\n",
    "\n",
    "    # allow shifting ag\n",
    "    natcrop_net_conversion = natural_crop_conversion_kernel.subtract(\n",
    "        natural_crop_reduction_kernel\n",
    "    ).max(0)\n",
    "    natcrop_conversion_by_human_lu = natural_crop_conversion_kernel.divide(human_lu).min(1).max(0)\n",
    "    natcrop_net_conversion_by_human_lu = natcrop_net_conversion.divide(human_lu).min(1).max(0)\n",
    "\n",
    "    # normalize to annual values\n",
    "    tree_loss_by_human_lu = tree_loss_by_human_lu.divide(20)\n",
    "    deforest_by_human_lu = deforest_by_human_lu.divide(20)\n",
    "    # convert to CO2\n",
    "    deforest_carbon_by_human_lu = deforest_carbon_by_human_lu.divide(20).multiply(3.66)\n",
    "\n",
    "    conversion_years = TARGET_YEAR - 2020  # 2020 is baseline year for natural land conversion\n",
    "    natcrop_conversion_by_human_lu = natcrop_conversion_by_human_lu.divide(conversion_years)\n",
    "    natcrop_net_conversion_by_human_lu = natcrop_net_conversion_by_human_lu.divide(conversion_years)\n",
    "\n",
    "    return {\n",
    "        \"deforest_by_human_lu\": deforest_by_human_lu,\n",
    "        \"deforest_carbon_by_human_lu\": deforest_carbon_by_human_lu,\n",
    "        \"tree_loss_by_human_lu\": tree_loss_by_human_lu,\n",
    "        \"natural_crop_conversion_by_human_lu\": natcrop_conversion_by_human_lu,\n",
    "        \"natural_crop_net_conversion_by_human_lu\": natcrop_net_conversion_by_human_lu,\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_viz_layers(indicator_layers=None):\n",
    "    indicator_layers = indicator_layers or compute_indicators()\n",
    "    cropland = get_cropland_2019()[\"cropland_2019\"]\n",
    "    nonnat = get_sbtn_layers()[\"nonnatural_excl_builtwater\"]\n",
    "    get_mask()\n",
    "\n",
    "    layers = {}\n",
    "    for k, image in indicator_layers.items():\n",
    "        layers[f\"cropland_{k}\"] = cropland.multiply(image)\n",
    "        layers[f\"nonnat_{k}\"] = nonnat.multiply(image)\n",
    "\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _cache_layers(layers, scale, reduce_resolution=False, default_scale=30):\n",
    "    \"\"\"Save or load cached layers, optionally downsampling with reduceResolution.\"\"\"\n",
    "    for k, image in layers.items():\n",
    "        asset_id = f\"{WORKING_FOLDER}/{k}_{scale}m\"\n",
    "        image = image.unmask()\n",
    "\n",
    "        if reduce_resolution:\n",
    "            _orig_scale = image.projection().nominalScale().getInfo()\n",
    "            if _orig_scale > 100000:  # GEE lost track of the scale, set it manually\n",
    "                image = image.setDefaultProjection(\n",
    "                    PROJECTION, None, default_scale\n",
    "                ).reduceResolution(\"mean\")\n",
    "            elif _orig_scale < scale:  #\n",
    "                image = image.reduceResolution(\"mean\")\n",
    "            # else: original scale is coarser than target scale, don't reduce resolution\n",
    "            _wait_for_ee_internal_cache(image)\n",
    "\n",
    "        layers[k] = eeUtil.findOrSaveImage(\n",
    "            image, asset_id, region=WORLD_GEOM, scale=scale, crs=PROJECTION, dtype=\"float\"\n",
    "        )\n",
    "    return layers\n",
    "\n",
    "\n",
    "def _wait_for_ee_internal_cache(image, retries=3):\n",
    "    \"\"\"Sometimes reduce resolution will fail due to there not being a default projection set,\n",
    "    even though we set one manually when it needs to be. This seems to be related to an\n",
    "    internal GEE caching issue.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        image.getInfo()\n",
    "    except EEException as e:\n",
    "        if e.args[0].startswith(\"Image.reduceResolution\"):\n",
    "            time.sleep(15)\n",
    "            if retries > 0:\n",
    "                _wait_for_ee_internal_cache(image, retries - 1)\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "\n",
    "def _export_layers(layers, scale=KERNEL_SCALE):\n",
    "    \"\"\"Export layers to GCS\"\"\"\n",
    "    for k, image in layers.items():\n",
    "        blob = f\"{EXPORT_PREFIX}/{k}_{scale}m.tif\"\n",
    "        eeUtil.exportImage(\n",
    "            image,\n",
    "            blob,\n",
    "            bucket=EXPORT_BUCKET,\n",
    "            region=WORLD_GEOM,\n",
    "            crs=PROJECTION,\n",
    "            scale=scale,\n",
    "            dtype=\"float\",\n",
    "        )\n",
    "\n",
    "\n",
    "def _check_layers_precomputed(layers):\n",
    "    \"\"\"Check if derived layers have been precomputed at ANALYSIS_SCALE\"\"\"\n",
    "    cached_assets = list(eeUtil.ls(f\"{WORKING_FOLDER}\"))\n",
    "    precomputed = all([f\"{k}_{ANALYSIS_SCALE}m\" in cached_assets for k in layers])\n",
    "    if not precomputed:\n",
    "        print(\"WARNING: Cannot compute next step until the following layers are computed.\")\n",
    "        print([k for k in layers if f\"{k}_{ANALYSIS_SCALE}m\" not in cached_assets])\n",
    "        print(\"Wait for current tasks to complete and then re-run this cell.\")\n",
    "        print(\"Current tasks:\", [t[\"description\"] for t in eeUtil.getTasks(True)])\n",
    "    return precomputed\n",
    "\n",
    "\n",
    "def compute_all_landscape_indicators(cache=True, export=False, kernel_radius=KERNEL_RADIUS):\n",
    "    \"\"\"Main function to compute all landscape indicators.\n",
    "\n",
    "    Use cache=True to compute and save layers, and return existing cached layers if they\n",
    "    already exist.\n",
    "\n",
    "    When saving layers, this function will need to be run multiple times. First to compute the\n",
    "    overlays at ANALYSIS_SCALE, then to compute the kernels and final indicators at KERNEL_SCALE.\n",
    "\n",
    "    (We can't run the computation all in one go, because there are pixel/memory limits for\n",
    "    kernel operations. We need to downsample to a lower resolution before the kernel,\n",
    "    but we don't want to downsample the input data before doing the overlay analysis.)\n",
    "    \"\"\"\n",
    "    cache = cache or export\n",
    "\n",
    "    input_layers = get_all_input_layers()\n",
    "    if cache:\n",
    "        derived_layers = _cache_layers(input_layers, ANALYSIS_SCALE, reduce_resolution=True)\n",
    "\n",
    "    derived_layers = compute_derived_layers(input_layers)\n",
    "\n",
    "    if cache and _check_layers_precomputed(ALL_INPUT_LAYERS):\n",
    "        derived_layers = _cache_layers(derived_layers, ANALYSIS_SCALE)\n",
    "\n",
    "    kernel_layers = compute_kernels(derived_layers, kernel_radius)\n",
    "    indicators = compute_indicators(kernel_layers)\n",
    "    viz_layers = compute_viz_layers(indicators)\n",
    "\n",
    "    if cache and _check_layers_precomputed(PRE_KERNEL_LAYERS):\n",
    "        rad_km = int(kernel_radius / 1000)\n",
    "        kernel_layers = _cache_layers(\n",
    "            {f\"{k}_{rad_km}km\": i for k, i in kernel_layers.items()}, KERNEL_SCALE\n",
    "        )\n",
    "        indicators = _cache_layers(\n",
    "            {f\"{k}_{rad_km}km\": i for k, i in indicators.items()}, KERNEL_SCALE\n",
    "        )\n",
    "        viz_layers = _cache_layers(\n",
    "            {f\"{k}_{rad_km}km\": i for k, i in viz_layers.items()}, KERNEL_SCALE\n",
    "        )\n",
    "\n",
    "        # export final layers to GCS\n",
    "        if export:\n",
    "            _export_layers(indicators, KERNEL_SCALE)\n",
    "            _export_layers(viz_layers, KERNEL_SCALE)\n",
    "\n",
    "    return {\n",
    "        **input_layers,\n",
    "        **derived_layers,\n",
    "        **kernel_layers,\n",
    "        **indicators,\n",
    "        **viz_layers,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preview all layers\n",
    "# You can preview all the calculations and intermediate layers below\n",
    "\n",
    "# The map will render quickly(ish) but is inacurate because many input datasets don't\n",
    "# have nearest-neighbor resampling or other masking issues in their pyramids/overviews.\n",
    "\n",
    "\n",
    "def visualize(layers):\n",
    "    map = gmap.Map()\n",
    "    map.add_basemap(\"SATELLITE\")\n",
    "    for k, layer in layers.items():\n",
    "        vmax = 1\n",
    "        if \"kernel\" in k:\n",
    "            vmax *= 0.2\n",
    "        if \"carbon\" in k:\n",
    "            vmax *= 50\n",
    "        if \"human_lu\" in k:\n",
    "            vmax *= 0.05\n",
    "        map.add_layer(\n",
    "            layer.updateMask(layer.divide(vmax)),\n",
    "            {\"min\": 0, \"max\": vmax * 2, \"palette\": [\"black\", \"red\", \"orange\", \"yellow\"]},\n",
    "            k,\n",
    "            False,\n",
    "        )\n",
    "    return map\n",
    "\n",
    "\n",
    "visualize(compute_all_landscape_indicators(cache=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and vizualize final layers\n",
    "\n",
    "# When caching the layers, we'll call reduceResolution to accurately\n",
    "# resample the input layers to lower resoultions.\n",
    "\n",
    "# NOTE: Computation needs to be done in three steps:\n",
    "#       First, we resample to 100m.\n",
    "#       Then we do the overlay analysis to identify deforestation, cropland expansion, etc.\n",
    "#       Then we'll use these outputs to comput the final kerneled layers at 1000m.\n",
    "#\n",
    "#       If the first step hasn't been run before, you'll note that you can't\n",
    "#       see kernel layers on the map (they time out computing over too many pixels).\n",
    "#       Wait for the tasks to complete and run the cell again.\n",
    "\n",
    "# NOTE: Sometimes this will raise an exception due to there not being a default\n",
    "#       projection set, even though we set one manually when it needs to be.\n",
    "#       This seems to be related to an internal GEE caching issue. If this happens,\n",
    "#       just run the cell again\n",
    "\n",
    "layers = compute_all_landscape_indicators(cache=True)\n",
    "\n",
    "visualize(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to GCS\n",
    "# compute_all_landscape_indicators(export=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "da3PwpnFFuhJ"
   },
   "outputs": [],
   "source": [
    "# make assets public\n",
    "eeUtil.setAcl(WORKING_FOLDER, \"public\", recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[f\"{t['state']} {t['description']}\" for t in eeUtil.getTasks()]\n",
    "# [ee.data.cancelTask(t['id']) for t in eeUtil.getTasks(active=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eeUtil.export([k for k in eeUtil.ls(WORKING_FOLDER, True) if k.endswith(\"50km_1000m\")],\n",
    "#    bucket=EXPORT_BUCKET, prefix=EXPORT_PREFIX)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
